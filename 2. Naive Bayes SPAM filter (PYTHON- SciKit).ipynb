{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data into a Python data frame\n",
    "df = pd.read_table(\"SMSSpamCollection\", header = None, names = [\"label\", \"text\"])\n",
    "\n",
    "# Pre-process the SMS messages\n",
    "# lower case\n",
    "data1= df.apply(lambda x: x.astype(str).str.lower())\n",
    "\n",
    "# removing punctuations\n",
    "data1[\"text\"] = data1[\"text\"].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# removing numbers\n",
    "data1[\"text\"] = data1[\"text\"].str.replace('[0-9]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle the messages and split them into a training set, validation set, and testing set         \n",
    "dfs = data1.sample(frac=1).reset_index(drop=True)\n",
    "training = data1.iloc[:2500, :]\n",
    "validation = data1.iloc[2500:3500, :]\n",
    "test = data1.iloc[3500:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a simple Naıve Bayes classiﬁer from scratch\n",
    "\n",
    "class NaiveBayesForSpam:\n",
    "    def train(self, hamMessages, spamMessages): \n",
    "        self.words = set(''.join(list(hamMessages) + list(spamMessages)).split())\n",
    "        self.priors = np.zeros(2)\n",
    "        self.priors[0] = float(len(hamMessages)) / (len(hamMessages) + len(spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        for i, w in enumerate(self.words):\n",
    "            prob1 = (1.0 + len([m for m in hamMessages if w in m])) / len(hamMessages) \n",
    "            prob2 = (1.0 + len([m for m in spamMessages if w in m])) / len(spamMessages) \n",
    "            self.likelihoods.append([min(prob1, 0.95) , min(prob2, 0.95)])\n",
    "        self.likelihoods = np.array(self.likelihoods).T\n",
    "\n",
    "    \n",
    "    def train2(self, hamMessages, spamMessages):\n",
    "        self.words = set(''.join(list(hamMessages) + list(spamMessages)).split())\n",
    "        self.priors = np.zeros(2)\n",
    "        self.priors[0] = float(len(hamMessages)) / (len(hamMessages) + len(spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0] \n",
    "        self.likelihoods = []\n",
    "        spamkeywords = [ ]\n",
    "        for i, w in enumerate(self.words):\n",
    "            prob1 = (1.0 + len([m for m in hamMessages if w in m])) / len(hamMessages)\n",
    "            prob2 = (1.0 + len([m for m in spamMessages if w in m])) / len(spamMessages) \n",
    "            if prob1 * 20 < prob2: \n",
    "                self.likelihoods.append([min(prob1 , 0.95) , min(prob2 , 0.95)])\n",
    "                spamkeywords.append(w) \n",
    "        self.words = spamkeywords\n",
    "        self.likelihoods = np.array(self.likelihoods).T\n",
    "        \n",
    "    def predict(self, message):\n",
    "        posteriors = np.copy(self.priors)\n",
    "        for i, w in enumerate(self.words):\n",
    "            if w in message.lower(): \n",
    "                posteriors *= self.likelihoods[:,i] \n",
    "            else:\n",
    "                posteriors *= np.ones(2) - self.likelihoods[:,i] \n",
    "            posteriors = posteriors / np.linalg.norm(posteriors, ord = 1) \n",
    "        if posteriors[0] > 0.5:\n",
    "            return ['ham', posteriors[0]]\n",
    "        return ['spam', posteriors[1]] \n",
    "\n",
    "    def score(self, messages, labels):\n",
    "        confusion = np.zeros(4).reshape(2, 2) \n",
    "        for m, l in zip(messages, labels):\n",
    "            if self.predict(m)[0] == 'ham' and l == 'ham': \n",
    "                confusion[0 ,0] += 1\n",
    "            elif self.predict(m)[0] == 'ham' and l == 'spam': \n",
    "                confusion[0 ,1] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'ham': \n",
    "                confusion[1 ,0] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'spam': \n",
    "                confusion[1 ,1] += 1\n",
    "        return (confusion[0,0] + confusion[1,1]) / float(confusion.sum()), confusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use your training set to train the classiﬁers ‘train’ and ‘train2’\n",
    "classiﬁer = NaiveBayesForSpam()\n",
    "\n",
    "classiﬁer.train(training[training[\"label\"] == \"ham\"][\"text\"], training[training[\"label\"] == \"spam\"][\"text\"])\n",
    "classiﬁer.train2(training[training[\"label\"] == \"ham\"][\"text\"], training[training[\"label\"] == \"spam\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using the validation set, explore how each of the two classiﬁers performs out of sample\n",
    "'''Using train Function'''\n",
    "# start timer\n",
    "start = timeit.default_timer() \n",
    "\n",
    "# using train function with training data set\n",
    "classiﬁer.train(training[training[\"label\"] == \"ham\"][\"text\"], training[training[\"label\"] == \"spam\"][\"text\"])\n",
    "\n",
    "# calculate accuracy and confusion matrix\n",
    "accuracy_1, confusion_matrix_1 = classiﬁer.score(pd.Series.tolist(validation[\"text\"]), pd.Series.tolist(validation[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.966 [[ 862.   20.]\n",
      " [  14.  104.]] 207.40554137714022\n"
     ]
    }
   ],
   "source": [
    "# stop timer and print\n",
    "stop = timeit.default_timer() \n",
    "print(accuracy_1, confusion_matrix_1, stop - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.969 [[ 871.   26.]\n",
      " [   5.   98.]] 7.11251009274892\n"
     ]
    }
   ],
   "source": [
    "'''Using train2 function'''\n",
    "# start timer\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# using train2 function with training data set\n",
    "classiﬁer.train2(training[training[\"label\"] == \"ham\"][\"text\"], training[training[\"label\"] == \"spam\"][\"text\"])\n",
    "\n",
    "# calculate accuracy and confusion matrix\n",
    "accuracy_2, confusion_matrix_2 = classiﬁer.score(pd.Series.tolist(validation[\"text\"]), pd.Series.tolist(validation[\"label\"]))\n",
    "\n",
    "# stop timer and print\n",
    "stop = timeit.default_timer()\n",
    "print(accuracy_2, confusion_matrix_2, stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.969594594595 [[ 1786.    55.]\n",
      " [    8.   223.]] 12.290132271242982\n"
     ]
    }
   ],
   "source": [
    "# Run the ‘train2’ classiﬁer on the test set and report its performance using a confusion matrix.\n",
    "start = timeit.default_timer()\n",
    "classiﬁer.train2(training[training[\"label\"] == \"ham\"][\"text\"], training[training[\"label\"] == \"spam\"][\"text\"])\n",
    "accuracy_3, confusion_matrix_3 = classiﬁer.score(pd.Series.tolist(test[\"text\"]), pd.Series.tolist(test[\"label\"]))\n",
    "stop = timeit.default_timer()\n",
    "print(accuracy_3, confusion_matrix_3, stop - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Train function:\n",
    "This is the first step of the learning process where we trained our model. The function essentially prepares available data to be used using Bayes Theorem. This is achieved by filling in tables for prior probability of each words being a spam or a ham. Then, it calculates likelihood based on total number of messages and frequency of words in these messages and build frequency table. The function also uses Laplace Estimator approach to ensure that each feature has a non-zero probability of occurring with each class; here 1 has been added. Similarly, it eliminates situations\n",
    "where probability equals 1 by replacing it with 0.95\n",
    "\n",
    "Train2 function:\n",
    "Train2 is almost identical to train function. The key difference is that it checks whether the probability of a word being spam is 20 times higher than being a ham, then that word will be assigned as spam keyword. Justification is to ensure that only strong candidate for a spam word\n",
    "will be identified as spam.\n",
    "\n",
    "Predict function:\n",
    "This function acted as part of score function (describe below). This function will look at the new message, checks if the words inside the message are in frequency table. If it is, then posteriors are being calculated; if not- it assigns values by subtracting likelihood from 1. The posterior values are being normalised and are classified as spam or ham based on its value comparing to 0.5.\n",
    "\n",
    "Score function:\n",
    "This function creates confusion matrix as part of a validation process by comparing the predicted labels against actual labels. This function uses ‘predict’ function described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed and accuracy of classifiers\n",
    "\n",
    "In case of ‘train’ function, the algorithm compares words of new messages to frequency tables resulted from training set. While ‘train 2’ function starts the algorithm in the same way, the difference comes where the function compares the probability of being SPAM against the probability of being HAM multiplied by 20. When such an event occurs, the function appends the word to the list of ‘spam’ key words. This list of key words is being used to identify whether words in new messages are spam or ham. The key efficiency comes from the fact that the algorithm instead of going through the large list of words and their probabilities (as the  case in ‘train’) for every word it is trying to classify, it goes through a shorter list of spam key words.\n",
    "\n",
    "In ‘train’ function- we get some words, which appear both in spam and ham messages, whereas in ‘train 2’ function it deals with words which have a very high likelihood of being spam.\n",
    "I also would like to note that in some instances train function gives a better accuracy, however in the long run (with higher iterations) the train2 will perform better."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
